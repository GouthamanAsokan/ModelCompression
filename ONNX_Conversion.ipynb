{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb5005db33c54cb49a79911b2d244542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b58acc0e8ec44690a0cae573cccbd5fb",
              "IPY_MODEL_8ac6094c0f5448579814456cb2249fcc",
              "IPY_MODEL_039cb1aaa24d4f55b23d8e03ca95d036"
            ],
            "layout": "IPY_MODEL_198a3ed65bdd4f3a8e7705e8a3c306de"
          }
        },
        "b58acc0e8ec44690a0cae573cccbd5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baae770fd66c4f0dbf8d0c2e6d246b7f",
            "placeholder": "​",
            "style": "IPY_MODEL_7d69aa2d52b1451690f0d6f66cd7a405",
            "value": "100%"
          }
        },
        "8ac6094c0f5448579814456cb2249fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bf91d95d46a4861b334aa07d3c635d2",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a6911d65b754dfe96fd8dc6ca868219",
            "value": 244408911
          }
        },
        "039cb1aaa24d4f55b23d8e03ca95d036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0f87cbd94044f8b7779d49b93de910",
            "placeholder": "​",
            "style": "IPY_MODEL_45be8aad7ed748ab8af2d7a6acf67696",
            "value": " 233M/233M [00:00&lt;00:00, 260MB/s]"
          }
        },
        "198a3ed65bdd4f3a8e7705e8a3c306de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baae770fd66c4f0dbf8d0c2e6d246b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d69aa2d52b1451690f0d6f66cd7a405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bf91d95d46a4861b334aa07d3c635d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6911d65b754dfe96fd8dc6ca868219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f0f87cbd94044f8b7779d49b93de910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45be8aad7ed748ab8af2d7a6acf67696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7nVQ9KFDDRb",
        "outputId": "0794c5f7-6820-41d6-b7a0-3e2b341b5f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function export in module torch.onnx.utils:\n",
            "\n",
            "export(model: 'Union[torch.nn.Module, torch.jit.ScriptModule, torch.jit.ScriptFunction]', args: 'Union[Tuple[Any, ...], torch.Tensor]', f: 'Union[str, io.BytesIO]', export_params: 'bool' = True, verbose: 'bool' = False, training: '_C_onnx.TrainingMode' = <TrainingMode.EVAL: 0>, input_names: 'Optional[Sequence[str]]' = None, output_names: 'Optional[Sequence[str]]' = None, operator_export_type: '_C_onnx.OperatorExportTypes' = <OperatorExportTypes.ONNX: 0>, opset_version: 'Optional[int]' = None, do_constant_folding: 'bool' = True, dynamic_axes: 'Optional[Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]]' = None, keep_initializers_as_inputs: 'Optional[bool]' = None, custom_opsets: 'Optional[Mapping[str, int]]' = None, export_modules_as_functions: 'Union[bool, Collection[Type[torch.nn.Module]]]' = False) -> 'None'\n",
            "    Exports a model into ONNX format.\n",
            "    \n",
            "    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\n",
            "    :class:`torch.jit.ScriptFunction`, this runs\n",
            "    ``model`` once in order to convert it to a TorchScript graph to be exported\n",
            "    (the equivalent of :func:`torch.jit.trace`). Thus this has the same limited support\n",
            "    for dynamic control flow as :func:`torch.jit.trace`.\n",
            "    \n",
            "    Args:\n",
            "        model (:class:`torch.nn.Module`, :class:`torch.jit.ScriptModule` or :class:`torch.jit.ScriptFunction`):\n",
            "            the model to be exported.\n",
            "        args (tuple or torch.Tensor):\n",
            "    \n",
            "            args can be structured either as:\n",
            "    \n",
            "            1. ONLY A TUPLE OF ARGUMENTS::\n",
            "    \n",
            "                args = (x, y, z)\n",
            "    \n",
            "            The tuple should contain model inputs such that ``model(*args)`` is a valid\n",
            "            invocation of the model. Any non-Tensor arguments will be hard-coded into the\n",
            "            exported model; any Tensor arguments will become inputs of the exported model,\n",
            "            in the order they occur in the tuple.\n",
            "    \n",
            "            2. A TENSOR::\n",
            "    \n",
            "                args = torch.Tensor([1])\n",
            "    \n",
            "            This is equivalent to a 1-ary tuple of that Tensor.\n",
            "    \n",
            "            3. A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS::\n",
            "    \n",
            "                args = (\n",
            "                    x,\n",
            "                    {\n",
            "                        \"y\": input_y,\n",
            "                        \"z\": input_z\n",
            "                    }\n",
            "                )\n",
            "    \n",
            "            All but the last element of the tuple will be passed as non-keyword arguments,\n",
            "            and named arguments will be set from the last element. If a named argument is\n",
            "            not present in the dictionary, it is assigned the default value, or None if a\n",
            "            default value is not provided.\n",
            "    \n",
            "            .. note::\n",
            "                If a dictionary is the last element of the args tuple, it will be\n",
            "                interpreted as containing named arguments. In order to pass a dict as the\n",
            "                last non-keyword arg, provide an empty dict as the last element of the args\n",
            "                tuple. For example, instead of::\n",
            "    \n",
            "                    torch.onnx.export(\n",
            "                        model,\n",
            "                        (\n",
            "                            x,\n",
            "                            # WRONG: will be interpreted as named arguments\n",
            "                            {y: z}\n",
            "                        ),\n",
            "                        \"test.onnx.pb\"\n",
            "                    )\n",
            "    \n",
            "                Write::\n",
            "    \n",
            "                    torch.onnx.export(\n",
            "                        model,\n",
            "                        (\n",
            "                            x,\n",
            "                            {y: z},\n",
            "                            {}\n",
            "                        ),\n",
            "                        \"test.onnx.pb\"\n",
            "                    )\n",
            "    \n",
            "        f: a file-like object (such that ``f.fileno()`` returns a file descriptor)\n",
            "            or a string containing a file name.  A binary protocol buffer will be written\n",
            "            to this file.\n",
            "        export_params (bool, default True): if True, all parameters will\n",
            "            be exported. Set this to False if you want to export an untrained model.\n",
            "            In this case, the exported model will first take all of its parameters\n",
            "            as arguments, with the ordering as specified by ``model.state_dict().values()``\n",
            "        verbose (bool, default False): if True, prints a description of the\n",
            "            model being exported to stdout. In addition, the final ONNX graph will include the\n",
            "            field ``doc_string``` from the exported model which mentions the source code locations\n",
            "            for ``model``. If True, ONNX exporter logging will be turned on.\n",
            "        training (enum, default TrainingMode.EVAL):\n",
            "            * ``TrainingMode.EVAL``: export the model in inference mode.\n",
            "            * ``TrainingMode.PRESERVE``: export the model in inference mode if model.training is\n",
            "                False and in training mode if model.training is True.\n",
            "            * ``TrainingMode.TRAINING``: export the model in training mode. Disables optimizations\n",
            "                which might interfere with training.\n",
            "        input_names (list of str, default empty list): names to assign to the\n",
            "            input nodes of the graph, in order.\n",
            "        output_names (list of str, default empty list): names to assign to the\n",
            "            output nodes of the graph, in order.\n",
            "        operator_export_type (enum, default OperatorExportTypes.ONNX):\n",
            "    \n",
            "            * ``OperatorExportTypes.ONNX``: Export all ops as regular ONNX ops\n",
            "                (in the default opset domain).\n",
            "            * ``OperatorExportTypes.ONNX_FALLTHROUGH``: Try to convert all ops\n",
            "                to standard ONNX ops in the default opset domain. If unable to do so\n",
            "                (e.g. because support has not been added to convert a particular torch op to ONNX),\n",
            "                fall back to exporting the op into a custom opset domain without conversion. Applies\n",
            "                to `custom ops <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_\n",
            "                as well as ATen ops. For the exported model to be usable, the runtime must support\n",
            "                these non-standard ops.\n",
            "            * ``OperatorExportTypes.ONNX_ATEN``: All ATen ops (in the TorchScript namespace \"aten\")\n",
            "                are exported as ATen ops (in opset domain \"org.pytorch.aten\").\n",
            "                `ATen <https://pytorch.org/cppdocs/#aten>`_ is PyTorch's built-in tensor library, so\n",
            "                this instructs the runtime to use PyTorch's implementation of these ops.\n",
            "    \n",
            "                .. warning::\n",
            "    \n",
            "                    Models exported this way are probably runnable only by Caffe2.\n",
            "    \n",
            "                    This may be useful if the numeric differences in implementations of operators are\n",
            "                    causing large differences in behavior between PyTorch and Caffe2 (which is more\n",
            "                    common on untrained models).\n",
            "    \n",
            "            * ``OperatorExportTypes.ONNX_ATEN_FALLBACK``: Try to export each ATen op\n",
            "                (in the TorchScript namespace \"aten\") as a regular ONNX op. If we are unable to do so\n",
            "                (e.g. because support has not been added to convert a particular torch op to ONNX),\n",
            "                fall back to exporting an ATen op. See documentation on OperatorExportTypes.ONNX_ATEN for\n",
            "                context.\n",
            "                For example::\n",
            "    \n",
            "                    graph(%0 : Float):\n",
            "                    %3 : int = prim::Constant[value=0]()\n",
            "                    # conversion unsupported\n",
            "                    %4 : Float = aten::triu(%0, %3)\n",
            "                    # conversion supported\n",
            "                    %5 : Float = aten::mul(%4, %0)\n",
            "                    return (%5)\n",
            "    \n",
            "                Assuming ``aten::triu`` is not supported in ONNX, this will be exported as::\n",
            "    \n",
            "                    graph(%0 : Float):\n",
            "                    %1 : Long() = onnx::Constant[value={0}]()\n",
            "                    # not converted\n",
            "                    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1)\n",
            "                    # converted\n",
            "                    %3 : Float = onnx::Mul(%2, %0)\n",
            "                    return (%3)\n",
            "    \n",
            "                If PyTorch was built with Caffe2 (i.e. with ``BUILD_CAFFE2=1``), then\n",
            "                Caffe2-specific behavior will be enabled, including special support\n",
            "                for ops are produced by the modules described in\n",
            "                `Quantization <https://pytorch.org/docs/stable/quantization.html>`_.\n",
            "    \n",
            "                .. warning::\n",
            "    \n",
            "                    Models exported this way are probably runnable only by Caffe2.\n",
            "    \n",
            "        opset_version (int, default 14): The version of the\n",
            "            `default (ai.onnx) opset <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_\n",
            "            to target. Must be >= 7 and <= 16.\n",
            "        do_constant_folding (bool, default True): Apply the constant-folding optimization.\n",
            "            Constant-folding will replace some of the ops that have all constant inputs\n",
            "            with pre-computed constant nodes.\n",
            "        dynamic_axes (dict[string, dict[int, string]] or dict[string, list(int)], default empty dict):\n",
            "    \n",
            "            By default the exported model will have the shapes of all input and output tensors\n",
            "            set to exactly match those given in ``args``. To specify axes of tensors as\n",
            "            dynamic (i.e. known only at run-time), set ``dynamic_axes`` to a dict with schema:\n",
            "    \n",
            "            * KEY (str): an input or output name. Each name must also be provided in ``input_names`` or\n",
            "                ``output_names``.\n",
            "            * VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If a\n",
            "                list, each element is an axis index.\n",
            "    \n",
            "            For example::\n",
            "    \n",
            "                class SumModule(torch.nn.Module):\n",
            "                    def forward(self, x):\n",
            "                        return torch.sum(x, dim=1)\n",
            "    \n",
            "                torch.onnx.export(\n",
            "                    SumModule(),\n",
            "                    (torch.ones(2, 2),),\n",
            "                    \"onnx.pb\",\n",
            "                    input_names=[\"x\"],\n",
            "                    output_names=[\"sum\"]\n",
            "                )\n",
            "    \n",
            "            Produces::\n",
            "    \n",
            "                input {\n",
            "                  name: \"x\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 0\n",
            "                        }\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 1\n",
            "                ...\n",
            "                output {\n",
            "                  name: \"sum\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 0\n",
            "                ...\n",
            "    \n",
            "            While::\n",
            "    \n",
            "                torch.onnx.export(\n",
            "                    SumModule(),\n",
            "                    (torch.ones(2, 2),),\n",
            "                    \"onnx.pb\",\n",
            "                    input_names=[\"x\"],\n",
            "                    output_names=[\"sum\"],\n",
            "                    dynamic_axes={\n",
            "                        # dict value: manually named axes\n",
            "                        \"x\": {0: \"my_custom_axis_name\"},\n",
            "                        # list value: automatic names\n",
            "                        \"sum\": [0],\n",
            "                    }\n",
            "                )\n",
            "    \n",
            "            Produces::\n",
            "    \n",
            "                input {\n",
            "                  name: \"x\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_param: \"my_custom_axis_name\"  # axis 0\n",
            "                        }\n",
            "                        dim {\n",
            "                          dim_value: 2  # axis 1\n",
            "                ...\n",
            "                output {\n",
            "                  name: \"sum\"\n",
            "                  ...\n",
            "                      shape {\n",
            "                        dim {\n",
            "                          dim_param: \"sum_dynamic_axes_1\"  # axis 0\n",
            "                ...\n",
            "    \n",
            "        keep_initializers_as_inputs (bool, default None): If True, all the\n",
            "            initializers (typically corresponding to parameters) in the\n",
            "            exported graph will also be added as inputs to the graph. If False,\n",
            "            then initializers are not added as inputs to the graph, and only\n",
            "            the non-parameter inputs are added as inputs.\n",
            "            This may allow for better optimizations (e.g. constant folding) by\n",
            "            backends/runtimes.\n",
            "    \n",
            "            If ``opset_version < 9``, initializers MUST be part of graph\n",
            "            inputs and this argument will be ignored and the behavior will be\n",
            "            equivalent to setting this argument to True.\n",
            "    \n",
            "            If None, then the behavior is chosen automatically as follows:\n",
            "    \n",
            "            * If ``operator_export_type=OperatorExportTypes.ONNX``, the behavior is equivalent\n",
            "                to setting this argument to False.\n",
            "            * Else, the behavior is equivalent to setting this argument to True.\n",
            "    \n",
            "        custom_opsets (dict[str, int], default empty dict): A dict with schema:\n",
            "    \n",
            "            * KEY (str): opset domain name\n",
            "            * VALUE (int): opset version\n",
            "    \n",
            "            If a custom opset is referenced by ``model`` but not mentioned in this dictionary,\n",
            "            the opset version is set to 1. Only custom opset domain name and version should be\n",
            "            indicated through this argument.\n",
            "    \n",
            "        export_modules_as_functions (bool or set of type of nn.Module, default False): Flag to enable\n",
            "            exporting all ``nn.Module`` forward calls as local functions in ONNX. Or a set to indicate the\n",
            "            particular types of modules to export as local functions in ONNX.\n",
            "            This feature requires ``opset_version`` >= 15, otherwise the export will fail. This is because\n",
            "            ``opset_version`` < 15 implies IR version < 8, which means no local function support.\n",
            "            Module variables will be exported as function attributes. There are two categories of function\n",
            "            attributes.\n",
            "    \n",
            "            1. Annotated attributes: class variables that have type annotations via\n",
            "            `PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_\n",
            "            will be exported as attributes.\n",
            "            Annotated attributes are not used inside the subgraph of ONNX local function because\n",
            "            they are not created by PyTorch JIT tracing, but they may be used by consumers\n",
            "            to determine whether or not to replace the function with a particular fused kernel.\n",
            "    \n",
            "            2. Inferred attributes: variables that are used by operators inside the module. Attribute names\n",
            "            will have prefix \"inferred::\". This is to differentiate from predefined attributes retrieved from\n",
            "            python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.\n",
            "    \n",
            "            * ``False`` (default): export ``nn.Module`` forward calls as fine grained nodes.\n",
            "            * ``True``: export all ``nn.Module`` forward calls as local function nodes.\n",
            "            * Set of type of nn.Module: export ``nn.Module`` forward calls as local function nodes,\n",
            "                only if the type of the ``nn.Module`` is found in the set.\n",
            "    \n",
            "    Raises:\n",
            "        :class:`torch.onnx.errors.CheckerError`: If the ONNX checker detects an invalid ONNX graph.\n",
            "        :class:`torch.onnx.errors.UnsupportedOperatorError`: If the ONNX graph cannot be exported because it\n",
            "            uses an operator that is not supported by the exporter.\n",
            "        :class:`torch.onnx.errors.OnnxExporterError`: Other errors that can occur during export.\n",
            "            All errors are subclasses of :class:`errors.OnnxExporterError`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.onnx\n",
        "help(torch.onnx.export)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting alexnet\n",
        "import torch.onnx\n",
        "import torchvision\n",
        "\n",
        "# Standard ImageNet input - 3 channels, 224x224,\n",
        "# values don't matter as we care about network structure.\n",
        "# But they can also be real inputs.\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "# Obtain your model, it can be also constructed in your script explicitly\n",
        "model = torchvision.models.alexnet(pretrained=True)\n",
        "# Invoke export\n",
        "torch.onnx.export(model, dummy_input, \"alexnet.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "eb5005db33c54cb49a79911b2d244542",
            "b58acc0e8ec44690a0cae573cccbd5fb",
            "8ac6094c0f5448579814456cb2249fcc",
            "039cb1aaa24d4f55b23d8e03ca95d036",
            "198a3ed65bdd4f3a8e7705e8a3c306de",
            "baae770fd66c4f0dbf8d0c2e6d246b7f",
            "7d69aa2d52b1451690f0d6f66cd7a405",
            "3bf91d95d46a4861b334aa07d3c635d2",
            "7a6911d65b754dfe96fd8dc6ca868219",
            "7f0f87cbd94044f8b7779d49b93de910",
            "45be8aad7ed748ab8af2d7a6acf67696"
          ]
        },
        "id": "ARpEFtIzDbOw",
        "outputId": "c02aa253-bef0-41d5-a642-5f1fc1c40a8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb5005db33c54cb49a79911b2d244542"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUjwN68ZDzYb",
        "outputId": "54b8ee42-7fcf-46e9-e59e-87403c4d4aa4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<4,>=3.20.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx) (4.4.0)\n",
            "Installing collected packages: protobuf, onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.13.0 protobuf-3.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#inspecting the model\n",
        "import onnx\n",
        "\n",
        "# Load the ONNX model\n",
        "model = onnx.load(\"alexnet.onnx\")\n",
        "\n",
        "# Check that the IR is well formed\n",
        "onnx.checker.check_model(model)\n",
        "\n",
        "# Print a human readable representation of the graph\n",
        "print(onnx.helper.printable_graph(model.graph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-4xSfG5DuSX",
        "outputId": "44c40ba7-1e6b-4299-94cf-dfd9b07ab7de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph torch_jit (\n",
            "  %input.1[FLOAT, 1x3x224x224]\n",
            ") initializers (\n",
            "  %features.0.weight[FLOAT, 64x3x11x11]\n",
            "  %features.0.bias[FLOAT, 64]\n",
            "  %features.3.weight[FLOAT, 192x64x5x5]\n",
            "  %features.3.bias[FLOAT, 192]\n",
            "  %features.6.weight[FLOAT, 384x192x3x3]\n",
            "  %features.6.bias[FLOAT, 384]\n",
            "  %features.8.weight[FLOAT, 256x384x3x3]\n",
            "  %features.8.bias[FLOAT, 256]\n",
            "  %features.10.weight[FLOAT, 256x256x3x3]\n",
            "  %features.10.bias[FLOAT, 256]\n",
            "  %classifier.1.weight[FLOAT, 4096x9216]\n",
            "  %classifier.1.bias[FLOAT, 4096]\n",
            "  %classifier.4.weight[FLOAT, 4096x4096]\n",
            "  %classifier.4.bias[FLOAT, 4096]\n",
            "  %classifier.6.weight[FLOAT, 1000x4096]\n",
            "  %classifier.6.bias[FLOAT, 1000]\n",
            ") {\n",
            "  %/features/features.0/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [11, 11], pads = [2, 2, 2, 2], strides = [4, 4]](%input.1, %features.0.weight, %features.0.bias)\n",
            "  %/features/features.1/Relu_output_0 = Relu(%/features/features.0/Conv_output_0)\n",
            "  %/features/features.2/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.1/Relu_output_0)\n",
            "  %/features/features.3/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%/features/features.2/MaxPool_output_0, %features.3.weight, %features.3.bias)\n",
            "  %/features/features.4/Relu_output_0 = Relu(%/features/features.3/Conv_output_0)\n",
            "  %/features/features.5/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.4/Relu_output_0)\n",
            "  %/features/features.6/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.5/MaxPool_output_0, %features.6.weight, %features.6.bias)\n",
            "  %/features/features.7/Relu_output_0 = Relu(%/features/features.6/Conv_output_0)\n",
            "  %/features/features.8/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.7/Relu_output_0, %features.8.weight, %features.8.bias)\n",
            "  %/features/features.9/Relu_output_0 = Relu(%/features/features.8/Conv_output_0)\n",
            "  %/features/features.10/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%/features/features.9/Relu_output_0, %features.10.weight, %features.10.bias)\n",
            "  %/features/features.11/Relu_output_0 = Relu(%/features/features.10/Conv_output_0)\n",
            "  %/features/features.12/MaxPool_output_0 = MaxPool[ceil_mode = 0, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%/features/features.11/Relu_output_0)\n",
            "  %/avgpool/AveragePool_output_0 = AveragePool[kernel_shape = [1, 1], strides = [1, 1]](%/features/features.12/MaxPool_output_0)\n",
            "  %/Flatten_output_0 = Flatten[axis = 1](%/avgpool/AveragePool_output_0)\n",
            "  %/classifier/classifier.1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/Flatten_output_0, %classifier.1.weight, %classifier.1.bias)\n",
            "  %/classifier/classifier.2/Relu_output_0 = Relu(%/classifier/classifier.1/Gemm_output_0)\n",
            "  %/classifier/classifier.4/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/classifier.2/Relu_output_0, %classifier.4.weight, %classifier.4.bias)\n",
            "  %/classifier/classifier.5/Relu_output_0 = Relu(%/classifier/classifier.4/Gemm_output_0)\n",
            "  %36 = Gemm[alpha = 1, beta = 1, transB = 1](%/classifier/classifier.5/Relu_output_0, %classifier.6.weight, %classifier.6.bias)\n",
            "  return %36\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}